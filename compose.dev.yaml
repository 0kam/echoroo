# Echoroo Development Docker Compose
# PostgreSQL + pgvector for ML embedding support
#
# Initial Setup (run once, or when dependencies change):
#   cd back && docker build -f Dockerfile.base -t echoroo-base:latest .
#
# Usage:
#   ./scripts/docker.sh dev        # Start development environment
#   ./scripts/docker.sh dev logs   # View logs
#   ./scripts/docker.sh dev stop   # Stop
#
# Features:
# - PostgreSQL with pgvector extension (port 5432 exposed for local tools)
# - Backend with hot reload via volume mount
# - Frontend with hot reload
# - Optional: Storybook and Docs servers

services:
  # PostgreSQL Database with pgvector
  db:
    image: pgvector/pgvector:pg16
    container_name: echoroo-db
    restart: unless-stopped
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - db-data:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/init-db.sql:ro
    environment:
      - POSTGRES_DB=${POSTGRES_DB:-echoroo}
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:?POSTGRES_PASSWORD must be set}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - echoroo-net

  # Redis for caching trained models during active learning
  redis:
    image: redis:7-alpine
    container_name: echoroo-redis
    restart: unless-stopped
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - echoroo-net

  # Backend API Server (using dev image with GPU-compatible TensorFlow)
  backend:
    build:
      context: back
      dockerfile: Dockerfile.dev
    container_name: echoroo-backend
    restart: unless-stopped
    command: ["python", "-m", "echoroo"]
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    ports:
      - "${ECHOROO_PORT:-5000}:5000"
    # Enable NVIDIA GPU support for ML inference
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      # Mount source code for hot reload - NO REBUILD NEEDED
      - ./back/src:/code/src:ro
      - ./back/README.md:/code/README.md:ro
      - ${ECHOROO_AUDIO_DIR:?ECHOROO_AUDIO_DIR must be set}:/audio:ro
      - backend-data:/data
      # Cache for ML models (BirdNET, Perch, etc.) - persists across restarts
      - ml-models:/root/.local/share
    environment:
      # PYTHONPATH to find the mounted source code
      - PYTHONPATH=/code/src
      - ECHOROO_HOST=0.0.0.0
      - ECHOROO_PORT=5000
      - ECHOROO_DOMAIN=${ECHOROO_DOMAIN:-localhost}
      - ECHOROO_AUDIO_DIR=/audio
      - ECHOROO_DB_DIALECT=postgresql
      - ECHOROO_DB_HOST=db
      - ECHOROO_DB_PORT=5432
      - ECHOROO_DB_NAME=${POSTGRES_DB:-echoroo}
      - ECHOROO_DB_USERNAME=${POSTGRES_USER:-postgres}
      - ECHOROO_DB_PASSWORD=${POSTGRES_PASSWORD}
      - ECHOROO_REDIS_HOST=redis
      - ECHOROO_REDIS_PORT=6379
      - ECHOROO_DEV=true
      - ECHOROO_LOG_TO_STDOUT=true
      - ECHOROO_LOG_TO_FILE=false
      - ECHOROO_OPEN_ON_STARTUP=false
      # GPU settings - read from .env file
      - ECHOROO_ML_GPU_BATCH_SIZE=${ECHOROO_ML_GPU_BATCH_SIZE:-16}
      - ECHOROO_ML_USE_GPU=${ECHOROO_ML_USE_GPU:-true}
      # Enable debug timing for ML inference performance analysis
      - ECHOROO_ML_DEBUG_TIMING=true
      # CUDA/TensorFlow settings for GPU support
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - TF_FORCE_GPU_ALLOW_GROWTH=true
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:5000/docs"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - echoroo-net

  # Frontend Next.js Server
  frontend:
    build:
      context: front
      target: development
    container_name: echoroo-frontend
    restart: unless-stopped
    command: ["npm", "run", "dev"]
    depends_on:
      backend:
        condition: service_healthy
    ports:
      - "${ECHOROO_FRONTEND_PORT:-3000}:3000"
    volumes:
      # Mount source for hot reload
      - ./front/src:/code/src:ro
      - ./front/public:/code/public:ro
      - ./front/next.config.js:/code/next.config.js:ro
    environment:
      - NEXT_PUBLIC_BACKEND_HOST=http://${ECHOROO_DOMAIN:-localhost}:${ECHOROO_PORT:-5000}
      - PORT=3000
    networks:
      - echoroo-net

  # Optional: Storybook (uncomment to enable)
  # storybook:
  #   build:
  #     context: front
  #     args:
  #       - NODE_ENV=development
  #   container_name: echoroo-storybook
  #   command: ["npm", "run", "storybook"]
  #   ports:
  #     - "6006:6006"
  #   develop:
  #     watch:
  #       - action: sync
  #         path: front/src
  #         target: /code/src

  # Optional: Documentation server (uncomment to enable)
  # docs:
  #   build:
  #     context: back
  #   container_name: echoroo-docs
  #   command: ["uv", "run", "mkdocs", "serve", "-a", "0.0.0.0:8000"]
  #   ports:
  #     - "8000:8000"
  #   develop:
  #     watch:
  #       - action: sync
  #         path: back/docs
  #         target: /code/docs

networks:
  echoroo-net:
    name: echoroo-dev

volumes:
  db-data:
    name: echoroo-dev-db
  backend-data:
    name: echoroo-dev-data
  ml-models:
    name: echoroo-dev-ml-models
  redis-data:
    name: echoroo-dev-redis
